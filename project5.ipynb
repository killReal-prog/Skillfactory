{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8fb4770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebb21c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/Users/kirill/Downloads/train 2.csv')\n",
    "df_test = pd.read_csv('/Users/kirill/Downloads/test.csv')\n",
    "sample_submission = pd.read_csv('/Users/kirill/Downloads/sample_submission.csv')\n",
    "\n",
    "# ВАЖНО! дря корректной обработки признаков объединяем трейн и тест в один датасет\n",
    "df_train['sample'] = 1 # помечаем где у нас трейн\n",
    "df_test['sample'] = 0  # помечаем где у нас тест\n",
    "df_test['default'] = 0 # в тесте у нас нет значения default, мы его должны предсказать, по этому пока просто заполняем нулями\n",
    "\n",
    "data = df_test.append(df_train, sort=False).reset_index(drop=True) # объединяем\n",
    "\n",
    "data.education.fillna('SCH', inplace=True)\n",
    "\n",
    "num_cols = ['age', 'score_bki', 'decline_app_cnt', 'score_bki', 'bki_request_cnt', 'income']\n",
    "cat_cols = ['education', 'first_time', 'sna', 'work_address', 'home_address', 'region_rating']\n",
    "bin_cols = ['sex', 'car', 'car_type', 'good_work', 'foreign_passport']\n",
    "\n",
    "'''# значимость числовых признаков\n",
    "from sklearn.feature_selection import f_classif\n",
    "imp_num = pd.Series(f_classif(data[num_cols], data['default'])[0], index = num_cols)\n",
    "imp_num.sort_values(inplace = True)\n",
    "imp_num.plot(kind = 'barh')\n",
    "\n",
    "# значимость категориальных признаков\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "imp_cat = pd.Series(mutual_info_classif(data[bin_cols + cat_cols], data['default'],\n",
    "                                     discrete_features =True), index = bin_cols + cat_cols)\n",
    "imp_cat.sort_values(inplace = True)\n",
    "imp_cat.plot(kind = 'barh')'''\n",
    "\n",
    "# преобразование категориальных признаков\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "for column in bin_cols:\n",
    "    data[column] = le.fit_transform(data[column])   \n",
    "for column in cat_cols:\n",
    "    data[column] = le.fit_transform(data[column])    \n",
    "for column in ['education', 'region_rating', 'home_address', 'sna']:\n",
    "    data = pd.get_dummies(data, columns = [column])\n",
    "\n",
    "# из коррелирующих признаков удаляем work_address и first_time, а так же удаляем ненужные client_id и app_date\n",
    "data.drop(['client_id','app_date', 'work_address', 'first_time'], axis = 1, inplace=True)\n",
    "\n",
    "# логарифмируем признак age\n",
    "data['age'] = np.log(data['age'] + 1)\n",
    "\n",
    "# логарифмируем признак decline_app_cnt\n",
    "data['decline_app_cnt'] = np.log(data['decline_app_cnt'] + 1)\n",
    "\n",
    "# логарифмируем признак income\n",
    "data['income'] = np.log(data['income'] + 1)\n",
    "\n",
    "# логарифмируем признак bki_request_cnt\n",
    "data['bki_request_cnt'] = np.log(data.bki_request_cnt + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02e99ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.69      0.69     12877\n",
      "           1       0.69      0.70      0.70     12894\n",
      "\n",
      "    accuracy                           0.70     25771\n",
      "   macro avg       0.70      0.70      0.70     25771\n",
      "weighted avg       0.70      0.70      0.70     25771\n",
      "\n",
      "ROC= 0.6954702930215668\n"
     ]
    }
   ],
   "source": [
    "# Теперь выделим тестовую часть\n",
    "train_data = data.query('sample == 1').drop(['sample'], axis=1)\n",
    "test_data = data.query('sample == 0').drop(['sample', 'default'], axis=1)\n",
    "\n",
    "y = train_data['default'].values  # наш таргет\n",
    "X = train_data.drop(['default'], axis=1)\n",
    "\n",
    "#Устраняем дисбаланс классов, что заметно улучшает метрику\n",
    "SM = SMOTE(sampling_strategy=1, random_state=42)\n",
    "X_balanced, y_balanced = SM.fit_resample(X, y)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)\n",
    "\n",
    "RS = RobustScaler()\n",
    "\n",
    "# Импортируем необходимые библиотеки:\n",
    "from sklearn.linear_model import LogisticRegression # инструмент для создания и обучения модели\n",
    "from sklearn import metrics # инструменты для оценки точности модели\n",
    "\n",
    "logreg = LogisticRegression(\n",
    "C=1.0\n",
    ",class_weight='balanced'\n",
    ",dual=False\n",
    ",fit_intercept=True\n",
    ",intercept_scaling=1\n",
    ",l1_ratio=None\n",
    ",max_iter=1000\n",
    ",multi_class='auto'\n",
    ",n_jobs=None\n",
    ",penalty='l2'\n",
    ",random_state=None\n",
    ",solver='sag'\n",
    ",tol=0.001\n",
    ",verbose=0\n",
    ",warm_start=False)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_valid)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "classification_report = classification_report(y_valid, y_pred)\n",
    "print(classification_report)\n",
    "print('ROC-AUC=',roc_auc_score(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f8d37da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kirill/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/Users/kirill/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "40 fits failed out of a total of 220.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kirill/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/kirill/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1528, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/Users/kirill/opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py\", line 1150, in _fit_liblinear\n",
      "    class_weight_ = compute_class_weight(class_weight, classes=classes_, y=y)\n",
      "  File \"/Users/kirill/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/class_weight.py\", line 58, in compute_class_weight\n",
      "    raise ValueError(\n",
      "ValueError: class_weight must be dict, 'balanced', or None, got: 'none'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kirill/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/kirill/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/kirill/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/kirill/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan 0.69671792        nan\n",
      " 0.69657433        nan 0.69666658 0.69670585        nan 0.69671085\n",
      " 0.69666464 0.69666658 0.69670585        nan 0.6967109  0.69665229\n",
      " 0.69666658 0.69667752 0.69659625 0.69673219 0.69667694 0.69666658\n",
      " 0.69667752 0.69659625 0.69672443 0.69669001 0.69642132 0.69639375\n",
      " 0.69646524 0.69644765 0.69642132 0.69639375 0.69647204 0.69644765\n",
      " 0.69642132 0.69644054 0.69646524 0.69644765 0.69642132 0.69644054\n",
      " 0.6964776  0.69646674]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tC: 1.0\n",
      "\tclass_weight: 'balanced'\n",
      "\tdual: False\n",
      "\tfit_intercept: True\n",
      "\tintercept_scaling: 1\n",
      "\tl1_ratio: None\n",
      "\tmax_iter: 1000\n",
      "\tmulti_class: 'auto'\n",
      "\tn_jobs: None\n",
      "\tpenalty: 'l2'\n",
      "\trandom_state: None\n",
      "\tsolver: 'sag'\n",
      "\ttol: 0.001\n",
      "\tverbose: 0\n",
      "\twarm_start: False\n"
     ]
    }
   ],
   "source": [
    "#подбираем гиперпараметры. Однако метрика не растет.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "iter_ = 1000\n",
    "epsilon_stop = 1e-3\n",
    "\n",
    "param_grid = [\n",
    "    {'penalty': ['l1'], \n",
    "     'solver': ['liblinear', 'lbfgs'], \n",
    "     'class_weight':['none', 'balanced'], \n",
    "     'multi_class': ['auto','ovr'], \n",
    "     'max_iter':[iter_],\n",
    "     'tol':[epsilon_stop]},\n",
    "    {'penalty': ['l2'], \n",
    "     'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], \n",
    "     'class_weight':['none', 'balanced'], \n",
    "     'multi_class': ['auto','ovr'], \n",
    "     'max_iter':[iter_],\n",
    "     'tol':[epsilon_stop]},\n",
    "    {'penalty': ['none'], \n",
    "     'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'], \n",
    "     'class_weight':['none', 'balanced'], \n",
    "     'multi_class': ['auto','ovr'], \n",
    "     'max_iter':[iter_],\n",
    "     'tol':[epsilon_stop]},\n",
    "]\n",
    "\n",
    "#model ваша модель логистической регрессии\n",
    "gridsearch = GridSearchCV(logreg, param_grid, scoring='f1', n_jobs=-1, cv=5)\n",
    "gridsearch.fit(X_train_balanced, y_train_balanced)\n",
    "model = gridsearch.best_estimator_\n",
    "\n",
    "#печатаем параметры\n",
    "best_parameters = model.get_params()\n",
    "for param_name in sorted(best_parameters.keys()):\n",
    "        print('\\t%s: %r' % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6658df13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Немного не в том порядке в этой ячейке визуализируем данные. Видим, что нужно прологарифмировать числовые признаки.\n",
    "from pandas_profiling import ProfileReport\n",
    "ProfileReport(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78d5bbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#теперь заполним тестовую выборку\n",
    "sample_submission['default'] = logreg.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5847bb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"sample_submission_result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfca28f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
